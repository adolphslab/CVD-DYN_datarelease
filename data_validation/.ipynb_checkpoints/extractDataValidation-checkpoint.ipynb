{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from valid_funct import *\n",
    "import importlib\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "parent_dir = Path().resolve().parents[0]\n",
    "data_dir = os.path.join(parent_dir, 'data')\n",
    "\n",
    "data = pd.read_csv(os.path.join(data_dir, 'Wave1-16_paper_release.csv'),\n",
    "                   encoding = 'ISO-8859-1',dtype=str, keep_default_na=False, \n",
    "                   na_values=['','NA'], low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# init output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = data[['CVDID', 'wave']].copy() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Check Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attDf = pd.DataFrame()\n",
    "n_w = list(data['wave'].unique())\n",
    "for wave in n_w:\n",
    "    tmp = attent_check(data, wave)\n",
    "    attDf = pd.concat([attDf, tmp], ignore_index=True, sort=False)\n",
    "valid_data = valid_data.merge(attDf, on = ['CVDID', 'wave'], how = 'left')\n",
    "\n",
    "check_for_errors(data, valid_data, 'attention questions')\n",
    "del attDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wave completed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_completed = w_completed(data)\n",
    "valid_data = valid_data.merge(w_completed, on = ['CVDID', 'wave'], how = 'left')\n",
    "\n",
    "check_for_errors(data, valid_data, 'wave completed')\n",
    "del w_completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free response questions: at least one noun or one verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nounVerb_counts = nounVerb_count(data)\n",
    "valid_data = valid_data.merge(nounVerb_counts, on = ['CVDID', 'wave'], how = 'left')\n",
    "\n",
    "check_for_errors(data, valid_data, 'noun verb count')\n",
    "del nounVerb_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR completion duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add completion time\n",
    "delta_t = pd.to_datetime(data['V4']) - pd.to_datetime(data['V3'])\n",
    "data['duration_in_s'] = delta_t.dt.total_seconds()/60\n",
    "iqr_time = interq_analysis(data[['CVDID','wave','duration_in_s']],3)\n",
    "iqr_time.rename(columns = {'duration_in_s': 'duration_outlier'}, inplace = True)\n",
    "iqr_time = iqr_time.replace({3.0: True, 2.0: False, 1.0: False, 0.0: False})\n",
    "valid_data = valid_data.merge(iqr_time, how = 'left', on = ['CVDID','wave'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR of mean response string length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string_df = data[['CVDID', 'wave']].copy()\n",
    "\n",
    "string_col_dict = {'PANA': data.loc[:,'PANA1_1':'PANA1_20'].columns,\n",
    "                'STAI_state': construct_vars('AnxS1_', 20),\n",
    "                'disgust_COVID': construct_vars('DISG1.2_', 22, skip=[13, 16]), \n",
    "                'fear_COVID': construct_vars('Fear2_', 7),\n",
    "                'EMSB': construct_vars('EMSB1_', 9),\n",
    "                'EMSC': construct_vars('EMSC1_', 9),\n",
    "                'NIHE':  construct_vars('NIHE1_', 8),\n",
    "                'NIHL': construct_vars('NIHL1_', 5),\n",
    "                'BDI': construct_vars('BDI', 21),\n",
    "                'CD_RISK': construct_vars('RISC1_', 10),\n",
    "                'consensBsl': construct_vars('cons1_', 35),\n",
    "                'consensCvd':construct_vars('cons2_', 20),\n",
    "                'EPII_infectHist_you':construct_vars('EPII10_', 8, '_1', skip=[7]),\n",
    "                'EPII_infectHist_inHome':construct_vars('EPII10_', 8, '_2', skip=[7]),\n",
    "                'EPII_infectHist_no':construct_vars('EPII10_', 8, '_3', skip=[7]),\n",
    "                'EPII_infectHist_NA': construct_vars('EPII10_', 8, '_4', skip=[7]),\n",
    "                'EPII_posChange_you': construct_vars('EPII11_', 18, '_1'),\n",
    "                'EPII_posChange_inHome': construct_vars('EPII11_', 18, '_2'),\n",
    "                'EPII_posChange_no': construct_vars('EPII11_', 18, '_3'), \n",
    "                'EPII_posChange_NA': construct_vars('EPII11_', 18, '_4'),\n",
    "                'EPII_work_you': construct_vars('EPII2_', 11, '_1'),\n",
    "                'EPII_work_inHome': construct_vars('EPII2_', 11, '_2'),\n",
    "                'EPII_work_no': construct_vars('EPII2_', 11, '_4'),\n",
    "                'EPII_work_NA': construct_vars('EPII2_', 11, '_5'),\n",
    "                'EPII_home_you': construct_vars('EPII4_', 13, '_1'),\n",
    "                'EPII_home_inHome': construct_vars('EPII4_', 13, '_2'),\n",
    "                'EPII_home_no': construct_vars('EPII4_', 13, '_3'),\n",
    "                'EPII_home_NA': construct_vars('EPII4_', 13, '_4'), \n",
    "                'EPII_socAct_you': construct_vars('EPII5_', 10, '_1'),\n",
    "                'EPII_socAct_inHome':construct_vars('EPII5_', 10, '_2'),\n",
    "                'EPII_socAct_no': construct_vars('EPII5_', 10, '_3'),\n",
    "                'EPII_socAct_NA': construct_vars('EPII5_', 10, '_4'),\n",
    "                'EPII_econ_you': construct_vars('EPII6_', 5, '_1'), \n",
    "                'EPII_econ_inHome': construct_vars('EPII6_', 5, '_2'), \n",
    "                'EPII_econ_no': construct_vars('EPII6_', 5, '_3'),\n",
    "                'EPII_econ_NA':  construct_vars('EPII6_', 5, '_4'),\n",
    "                'EPII_emo_you': construct_vars('EPII7_', 8, '_1', skip=[1,2]),\n",
    "                'EPII_emo_inHome': construct_vars('EPII7_', 8, '_2', skip=[1,2]),\n",
    "                'EPII_emo_no': construct_vars('EPII7_', 8, '_3'),\n",
    "                'EPII_emo_no': construct_vars('EPII7_', 8, '_4'),\n",
    "                'EPII_phys_you': construct_vars('EPII8_', 8, '_1'),\n",
    "                'EPII_phys_inHome': construct_vars('EPII8_', 8, '_2'),\n",
    "                'EPII_phys_no': construct_vars('EPII8_', 8, '_3'),\n",
    "                'EPII_phys_NA': construct_vars('EPII8_', 8, '_4'),\n",
    "                'EPII_dist_you': construct_vars('EPII9_', 8, '_1', skip=[8]),\n",
    "                'EPII_dist_inHome': construct_vars('EPII9_', 8, '_2', skip=[8]), \n",
    "                'EPII_dist_no': construct_vars('EPII9_', 8, '_3'), \n",
    "                'EPII_dist_NA': construct_vars('EPII9_', 8, '_4'),  \n",
    "                'trustPolit': construct_vars('RW6_1_', 11,skip=[7]),\n",
    "                'CovidImpact_inHouse': construct_vars('RW21_1_', 6),     \n",
    "                'CovidImpact_inHouse_v2': construct_vars('RW21v2_1_', 6),\n",
    "                'CovidImpact_hasCvd' : construct_vars('RW21_2_', 6) ,\n",
    "                'CovidImpact_hasCvd_v2': construct_vars('RW21v2_2_', 6),\n",
    "                'CovidImpact_posTest':  construct_vars('RW21_3_', 6), \n",
    "                'CovidImpact_posTest_v2': construct_vars('RW21v2_3_', 6),\n",
    "                'CovidImpact_hospital':  construct_vars('RW21_4_', 6), \n",
    "                'CovidImpact_hospital_v2':  construct_vars('RW21v2_4_', 6),\n",
    "                'CovidImpact_deceased':  construct_vars('RW21_5_', 6), \n",
    "                'CovidImpact_deceased_v2': construct_vars('RW21v2_5_', 6),\n",
    "                'CovidImpact_NA':  construct_vars('RW21_6_', 6), \n",
    "                'CovidImpact_NA_v2': construct_vars('RW21v2_6_', 6),\n",
    "                'approve': construct_vars('RW25_', 8), \n",
    "                'CvdPrevent': construct_vars('RW26_', 18),\n",
    "                'protest2': construct_vars('GFPS2_', 11, skip=[6]),\n",
    "                'protest2_v2': construct_vars('GFPS2v2_', 10, skip=[6]),\n",
    "                'protest4': construct_vars('GFPS4_', 7, skip=[3,5]),\n",
    "                'protest4_v2': construct_vars('GFPS4v2_', 7, skip=[3,5]),\n",
    "                'protest5': construct_vars('GFPS5_', 7, skip=[6]),\n",
    "                'protest5_v2': construct_vars('GFPS5v2_', 7, skip=[6]),\n",
    "                'protest6_happened': construct_vars('GFPS6_', 12, '_1', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_happened_v2': construct_vars('GFPS6v2_', 12, '_1', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_witnessed': construct_vars('GFPS6_', 12, '_3', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_witnessed_v2': construct_vars('GFPS6v2_', 12, '_3', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_learned': construct_vars('GFPS6_', 12, '_4', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_learned_v2': construct_vars('GFPS6v2_', 12, '_4', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_no': construct_vars('GFPS6_', 12, '_5', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_no_v2': construct_vars('GFPS6v2_', 12, '_5', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_noDisclose': construct_vars('GFPS6_', 12, '_6', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_noDisclose_v2': construct_vars('GFPS6v2_', 12, '_6', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_NA': construct_vars('GFPS6_', 12, '_7', skip=[1,2,3,5,6,8]),\n",
    "                'protest6_NA_v2': construct_vars('GFPS6v2_', 12, '_7', skip=[1,2,3,5,6,8]),\n",
    "                'protest9': construct_vars('GFPS9_', 7),\n",
    "                'protest10': construct_vars('GFPS10_', 8),\n",
    "                'protest11':construct_vars('GFPS11_', 7),\n",
    "                'protest12':construct_vars('GFPS12_', 8),\n",
    "                'protest12_v2':construct_vars('GFPS12v2_', 8),\n",
    "                'protest13': construct_vars('GFPS13_', 7),\n",
    "                'protest14': construct_vars('GFPS14_', 8),\n",
    "                'protest15': construct_vars('GFPS15_', 7),\n",
    "                'protest16': construct_vars('GFPS16_', 8),\n",
    "                'protest17': construct_vars('GFPS17_', 12, skip=[4,5,7]),\n",
    "                'protest17_v2': construct_vars('GFPS17v2_', 12, skip=[4,5,7]),\n",
    "                'protest33': construct_vars('GFPS33_', 5),\n",
    "                'EES': construct_vars('EES1_', 31),\n",
    "                'Dscr': construct_vars('Dscr1_', 9),\n",
    "                'FWI': construct_vars('FWI1_', 15),\n",
    "                'hum': construct_vars('Hum1_', 10),\n",
    "                'PC': construct_vars('PC5.2_', 5),\n",
    "                'PPK': construct_vars('CvPP1_', 12),\n",
    "                'ResSe': construct_vars('ReSe1_', 22),\n",
    "                'SPS_city': construct_vars('City_', 12),\n",
    "                'SPS_state': construct_vars('State_', 12),\n",
    "                'SPS_fed': construct_vars('Fed_', 12),\n",
    "                'STAI_trait':  construct_vars('AnxT_', 20),\n",
    "                'VSA': construct_vars('VSA1_', 6),\n",
    "                'NEO1': construct_vars('NEO1_', 10),\n",
    "                'NEO2': construct_vars('NEO2_', 10),\n",
    "                'NEO3': construct_vars('NEO3_', 10),\n",
    "                'NEO4': construct_vars('NEO4_', 10),\n",
    "                'NEO5': construct_vars('NEO5_', 10),\n",
    "                'NEO6': construct_vars('NEO6_', 10),\n",
    "                'LEC_me': construct_vars('LEC1_', 17, '_1'),\n",
    "                'LEC_me_May': construct_vars('LEC1_May_', 17, '_1'),\n",
    "                'LEC_witness': construct_vars('LEC1_May_', 17, '_2'),\n",
    "                'LEC_witness_May': construct_vars('LEC1_', 17, '_2'),\n",
    "                'LEC_learned': construct_vars('LEC1_May_', 17, '_3'),\n",
    "                'LEC_learned_May': construct_vars('LEC1_', 17, '_3'),\n",
    "                'LEC_job': construct_vars('LEC1_', 17, '_4'),\n",
    "                'LEC_job_May': construct_vars('LEC1_May_', 17, '_4'),\n",
    "                'LEC_notSure': construct_vars('LEC1_', 17, '_5'),\n",
    "                'LEC_notSure_May': construct_vars('LEC1_May_', 17, '_5'),\n",
    "                'LEC_NA': construct_vars('LEC1_', 17, '_6'),\n",
    "                'LEC_NA_May': construct_vars('LEC1_May_', 17, '_6'),\n",
    "                'Cnsp': data.loc[:,'Cnsp1_1':'Cnsp4_8'].columns}\n",
    "for name in list(string_col_dict.keys()):\n",
    "    input_vars = string_col_dict[name]\n",
    "    long_string_df = long_string_df = extract_long_string(long_string_df, data, name, input_vars)\n",
    "\n",
    "iqr_rs_mean_cols = ['CVDID', 'wave'] +  list(long_string_df.columns[long_string_df.columns.str.startswith('meanLongString')])\n",
    "iqr_rs_mean = long_string_df[iqr_rs_mean_cols]\n",
    "iqr_rs_mean = interq_analysis(iqr_rs_mean, 3)\n",
    "iqr_rs_mean = iqr_rs_mean.replace({3.0: True, 2.0: False, 1.0: False, 0.0: False})\n",
    "\n",
    "# core questionnaires\n",
    "core_rs_questionnaires = ['meanLongString_PANA', 'meanLongString_STAI_state', 'meanLongString_STAI_trait', \n",
    "                          'meanLongString_NEO1', 'meanLongString_NEO2', 'meanLongString_NEO3', 'meanLongString_NEO4', \n",
    "                          'meanLongString_NEO5', 'meanLongString_NEO6', 'meanLongString_VSA', 'meanLongString_EES']\n",
    "\n",
    "# all questionnaires with >=4 questions\n",
    "all_rs_questionnaires = long_string_df.columns[long_string_df.columns.str.startswith('meanLongString')]\n",
    "iqr_rs_mean['string_outlier_core'] = iqr_rs_mean[core_rs_questionnaires].mean(axis=1, skipna = True)>=0.5\n",
    "iqr_rs_mean['string_outlier_all'] = iqr_rs_mean[all_rs_questionnaires].mean(axis=1, skipna = True)>=0.5\n",
    "\n",
    "valid_data = valid_data.merge(iqr_rs_mean[['CVDID','wave','string_outlier_core', 'string_outlier_all']], how = 'left', on = ['CVDID','wave'])\n",
    "\n",
    "check_for_errors(data, valid_data, 'questionnaire string outlier')\n",
    "del iqr_rs_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR of within questionnaire correlations: pos/neg items and regular and reverse scored items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_df = data[['CVDID', 'wave']].copy()\n",
    "\n",
    "PANAS_pos_idx = ['PANA1_1','PANA1_3','PANA1_5','PANA1_9','PANA1_10','PANA1_12','PANA1_14','PANA1_16',\n",
    "                 'PANA1_17','PANA1_19']\n",
    "PANAS_neg_idx = ['PANA1_2','PANA1_4','PANA1_6','PANA1_7','PANA1_8','PANA1_11','PANA1_13','PANA1_15',\n",
    "                 'PANA1_18','PANA1_20']\n",
    "\n",
    "PSS_idx = ['PSS1','PSS2','PSS3','PSS4','PSS5','PSS6','PSS7','PSS8','PSS9','PSS10']\n",
    "\n",
    "STAI_rev = ['AnxS1_1','AnxS1_2','AnxS1_5','AnxS1_8','AnxS1_10','AnxS1_11','AnxS1_15','AnxS1_16','AnxS1_19','AnxS1_20']\n",
    "STAI_reg = ['AnxS1_3', 'AnxS1_4', 'AnxS1_6', 'AnxS1_7', 'AnxS1_9', 'AnxS1_12', 'AnxS1_13','AnxS1_14', 'AnxS1_17','AnxS1_18']\n",
    "\n",
    "EES_empFeelExp_reg = ['EES1_3','EES1_9', 'EES1_11', 'EES1_12', 'EES1_13', 'EES1_14', 'EES1_15', 'EES1_18','EES1_22','EES1_23','EES1_26', 'EES1_30']\n",
    "EES_empFeelExp_rev = ['EES1_16','EES1_17', 'EES1_21']\n",
    "\n",
    "EES_empPers_reg = ['EES1_4','EES1_6', 'EES1_19']\n",
    "EES_empPers_rev = ['EES1_2','EES1_28', 'EES1_29', 'EES1_31']\n",
    "\n",
    "Disg_reg = ['DISG1.1_2', 'DISG1.1_3','DISG1.1_4', 'DISG1.1_5', 'DISG1.1_7', 'DISG1.1_8','DISG1.1_9', 'DISG1.1_12', 'DISG1.1_14', 'DISG1.1_16',\n",
    "            'DISG1.2_1', 'DISG1.2_3', 'DISG1.2_4', 'DISG1.2_5', 'DISG1.2_6', 'DISG1.2_7', 'DISG1.2_8', 'DISG1.2_9', 'DISG1.2_10', 'DISG1.2_11',\n",
    "            'DISG1.2_12', 'DISG1.2_14']\n",
    "Disg_rev = ['DISG1.1_1', 'DISG1.1_6','DISG1.1_10', 'DISG1.1_13', 'DISG1.2_2']\n",
    "\n",
    "coherence_df['PANAS_diff'] = abs(data[PANAS_neg_idx].astype(float).mean(axis=1) - data[PANAS_pos_idx].astype(float).mean(axis=1))\n",
    "coherence_df['PANASpos_PSS_diff'] = abs(data[PSS_idx].astype(float).mean(axis=1) - data[PANAS_pos_idx].astype(float).mean(axis=1))\n",
    "coherence_df['STAI_diff'] = abs(data[STAI_rev].astype(float).mean(axis=1) - data[STAI_reg].astype(float).mean(axis=1))\n",
    "coherence_df['EES_empFeelExp_diff'] = abs(data[EES_empFeelExp_rev].astype(float).mean(axis=1) - data[EES_empFeelExp_reg].astype(float).mean(axis=1))\n",
    "coherence_df['EES_empPers_diff'] = abs(data[EES_empPers_rev].astype(float).mean(axis=1) - data[EES_empPers_reg].astype(float).mean(axis=1))\n",
    "coherence_df['Disg_diff'] = abs(data[Disg_rev].astype(float).mean(axis=1) - data[Disg_reg].astype(float).mean(axis=1))\n",
    "\n",
    "iqr_coherence = interq_analysis(coherence_df, 3)\n",
    "iqr_coherence = iqr_coherence.replace({3.0: True, 2.0: False,1.0:  False, 0.0: False})\n",
    "iqr_coherence['response_consistency'] = iqr_coherence[['PANAS_diff','PANASpos_PSS_diff','STAI_diff',\n",
    "                                         'EES_empFeelExp_diff', 'EES_empPers_diff','Disg_diff']].mean(axis=1, skipna = True)>=0.5\n",
    "\n",
    "valid_data = valid_data.merge(iqr_coherence[['CVDID','wave','response_consistency']], how = 'left', on = ['CVDID','wave'])\n",
    "\n",
    "check_for_errors(data, valid_data, 'questionnaire coherence')\n",
    "del iqr_coherence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of nan responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trust opinions -> not sure\n",
    "tmp = pd.DataFrame()\n",
    "nanResp_df = pd.DataFrame()\n",
    "nanResp_df = data[['CVDID', 'wave']].copy()\n",
    "\n",
    "tmp = data[['RW6_1_1','RW6_1_2', 'RW6_1_3', 'RW6_1_4', 'RW6_1_5', 'RW6_1_6', 'RW6_1_8', 'RW6_1_9', 'RW6_1_10', 'RW6_1_11']] == '7.0'\n",
    "tmp[data[['RW6_1_1','RW6_1_2', 'RW6_1_3', 'RW6_1_4', 'RW6_1_5', 'RW6_1_6', 'RW6_1_8', 'RW6_1_9', 'RW6_1_10', 'RW6_1_11']].isnull()] = np.nan\n",
    "nanResp_df['NaN_Freq1'] = tmp.mean(axis = 1)\n",
    "del tmp\n",
    "\n",
    "# important to prevent the spread of Covid -> not sure\n",
    "tmp = pd.DataFrame()\n",
    "tmp = data[['RW25_1', 'RW25_2', 'RW25_3', 'RW25_4', 'RW25_5', 'RW25_6', 'RW25_7', 'RW25_8']] == '7.0'\n",
    "tmp[data[['RW25_1', 'RW25_2', 'RW25_3', 'RW25_4', 'RW25_5', 'RW25_6', 'RW25_7', 'RW25_8']].isnull()] = np.nan\n",
    "nanResp_df['NaN_Freq2'] = tmp.mean(axis = 1)\n",
    "del tmp\n",
    "\n",
    "# Public Policy knowledge -> I don't know\n",
    "tmp = pd.DataFrame()\n",
    "tmp = data[['CvPP1_1','CvPP1_2','CvPP1_3','CvPP1_4','CvPP1_5','CvPP1_6','CvPP1_7','CvPP1_8','CvPP1_9','CvPP1_10','CvPP1_11','CvPP1_12']] == '4.0'\n",
    "tmp[data[['CvPP1_1','CvPP1_2','CvPP1_3','CvPP1_4','CvPP1_5','CvPP1_6','CvPP1_7','CvPP1_8','CvPP1_9','CvPP1_10','CvPP1_11','CvPP1_12']].isnull()] = np.nan\n",
    "nanResp_df['NaN_Freq3'] = tmp.mean(axis = 1)\n",
    "del tmp\n",
    "\n",
    "iqr_nanResp = interq_analysis(nanResp_df, 3)\n",
    "iqr_nanResp = iqr_nanResp.replace({3.0: True, 2.0: False,1.0:  False, 0.0: False})\n",
    "# NAN resp in half or more than half of the questions including NAN responses\n",
    "iqr_nanResp['freq_NAresp'] = iqr_nanResp[['NaN_Freq1','NaN_Freq2','NaN_Freq3']].mean(axis=1, skipna = True)>=0.5\n",
    "\n",
    "# add to validation data\n",
    "valid_data = valid_data.merge(iqr_nanResp[['CVDID','wave','freq_NAresp']], how = 'left', on = ['CVDID','wave'])\n",
    "\n",
    "check_for_errors(data, valid_data, 'NaN resps')\n",
    "del iqr_nanResp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overview over NA-responses per wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanResp_df = data[['CVDID', 'wave']].copy()\n",
    "\n",
    "nanResp_df[['RW6_1_1','RW6_1_2', 'RW6_1_3', 'RW6_1_4', 'RW6_1_5', \n",
    "            'RW6_1_6', 'RW6_1_8', 'RW6_1_9', 'RW6_1_10', \n",
    "            'RW6_1_11']] = data[['RW6_1_1','RW6_1_2', 'RW6_1_3', 'RW6_1_4', 'RW6_1_5', \n",
    "            'RW6_1_6', 'RW6_1_8', 'RW6_1_9', 'RW6_1_10', 'RW6_1_11']] == '7.0'\n",
    "nanResp_df[data[['RW6_1_1','RW6_1_2', 'RW6_1_3', 'RW6_1_4', 'RW6_1_5', \n",
    "            'RW6_1_6', 'RW6_1_8', 'RW6_1_9', 'RW6_1_10', \n",
    "            'RW6_1_11']].isnull()] = np.nan\n",
    "\n",
    "nanResp_df[['RW25_1', 'RW25_2', 'RW25_3', 'RW25_4', 'RW25_5', 'RW25_6', 'RW25_7', \n",
    "            'RW25_8']]= data[['RW25_1', 'RW25_2', 'RW25_3', 'RW25_4', 'RW25_5', \n",
    "                              'RW25_6', 'RW25_7', 'RW25_8']] == '7.0'\n",
    "nanResp_df[data[['RW25_1', 'RW25_2', 'RW25_3', 'RW25_4', 'RW25_5', 'RW25_6', 'RW25_7','RW25_8']].isnull()] = np.nan\n",
    "\n",
    "nanResp_df[['CvPP1_1','CvPP1_2','CvPP1_3','CvPP1_4','CvPP1_5','CvPP1_6',\n",
    "            'CvPP1_7','CvPP1_8','CvPP1_9','CvPP1_10','CvPP1_11',\n",
    "            'CvPP1_12']]= data[['CvPP1_1','CvPP1_2','CvPP1_3','CvPP1_4',\n",
    "                                'CvPP1_5','CvPP1_6','CvPP1_7','CvPP1_8','CvPP1_9',\n",
    "                                'CvPP1_10','CvPP1_11','CvPP1_12']] == '4.0'\n",
    "nanResp_df[data[['CvPP1_1','CvPP1_2','CvPP1_3','CvPP1_4','CvPP1_5','CvPP1_6',\n",
    "            'CvPP1_7','CvPP1_8','CvPP1_9','CvPP1_10','CvPP1_11','CvPP1_12']].isnull()] = np.nan\n",
    "\n",
    "\n",
    "nanResp_df['sum_NA_resp'] = nanResp_df.iloc[:,2:].sum(axis = 1)\n",
    "nanResp_df['n_NA_resp'] = np.sum(np.array(~nanResp_df[list(nanResp_df.columns[2:-1])].isnull()),1)\n",
    "nanResp_df.loc[nanResp_df['sum_NA_resp'] > nanResp_df['n_NA_resp']/2,'N_overX'] = 1\n",
    "\n",
    "nanResp_summary_all = pd.DataFrame(index = nanResp_df.wave.unique()) \n",
    "nanResp_summary_core = pd.DataFrame(index = nanResp_df.wave.unique()) \n",
    "\n",
    "# NA summary all subjects\n",
    "nanResp_summary_all['mean'] = nanResp_df.loc[:,['wave','sum_NA_resp']].groupby(by='wave').mean()\n",
    "nanResp_summary_all['var'] = nanResp_df.loc[:,['wave','sum_NA_resp']].groupby(by='wave').var()\n",
    "nanResp_summary_all['max'] = nanResp_df.loc[:,['wave','sum_NA_resp']].groupby(by='wave').max()\n",
    "nanResp_summary_all['>half'] = nanResp_df.loc[:,['wave','N_overX']].groupby(by='wave').sum()\n",
    "nanResp_summary_all['N'] = nanResp_df.loc[:,['wave','n_NA_resp']].groupby(by='wave').max()\n",
    "\n",
    "\n",
    "# # NA summary core sample \n",
    "nanResp_df =  nanResp_df.loc[nanResp_df.CVDID.isin(data.loc[data['sample'] == 'core', 'CVDID'])]\n",
    "nanResp_summary_core['mean'] = nanResp_df.loc[:,['wave','sum_NA_resp']].groupby(by='wave').mean()\n",
    "nanResp_summary_core['var'] = nanResp_df.loc[:,['wave','sum_NA_resp']].groupby(by='wave').var()\n",
    "nanResp_summary_core['max'] = nanResp_df.loc[:,['wave','sum_NA_resp']].groupby(by='wave').max()\n",
    "nanResp_summary_core['>half'] = nanResp_df.loc[:,['wave','N_overX']].groupby(by='wave').sum()\n",
    "nanResp_summary_core['N'] = nanResp_df.loc[:,['wave','n_NA_resp']].groupby(by='wave').max()\n",
    "nanResp_summary_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response string length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% run ProcessTaskData.ipynb\n",
    "task_data = pd.read_csv(os.path.join(data_dir, 'task_data', 'task_qual.csv'),dtype=str, low_memory=False)\n",
    "\n",
    "rs_cols = list(task_data.columns[task_data.columns.str.endswith('_meanLongString')])\n",
    "task_data[rs_cols] =task_data[rs_cols].astype(float)\n",
    "rs_cols = ['CVDID','wave'] + rs_cols \n",
    "\n",
    "iqr_task_rs = interq_analysis(task_data[rs_cols],3)\n",
    "iqr_task_rs = iqr_task_rs.replace({3.0: True, 2.0: False,1.0:  False, 0.0: False})\n",
    "iqr_task_rs  = iqr_task_rs.rename(columns={\"tr_1s_meanLongString\": \"string_outlier_tr\", \n",
    "                                            \"amp_meanLongString\": \"string_outlier_amp\",\n",
    "                                            \"altt_meanLongString\": \"string_outlier_altt\",\n",
    "                                            \"cvd_consp_meanLongString\": \"string_outlier_cvd_consp\"})\n",
    "\n",
    "\n",
    "\n",
    "valid_data = valid_data.merge(iqr_task_rs, how = 'left', on = ['CVDID','wave'])\n",
    "\n",
    "check_for_errors(data, valid_data, 'task rs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other task quality measures: RT outliers, iat/biat exclusion, tasks missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl_bad_rt_cutoff = 0.1\n",
    "\n",
    "task_valid = task_data[['CVDID','wave']].copy()\n",
    "\n",
    "# read in additional task quality measures: \n",
    "task_valid.loc[task_data['tr_1s_rt_pctlt_300'].astype(float)>=pctl_bad_rt_cutoff,'tr_1s_rt_pctlt_300'] = True\n",
    "task_valid.loc[task_data['tr_1s_rt_pctlt_300'].astype(float)<pctl_bad_rt_cutoff,'tr_1s_rt_pctlt_300'] = False\n",
    "task_valid.loc[task_data['tr_1s_rt_pctlt_300'].isnull(),'tr_1s_rt_pctlt_300'] = np.nan\n",
    "\n",
    "task_valid.loc[task_data['altt_rt_pctlt_300'].astype(float)>=pctl_bad_rt_cutoff,'altt_rt_pctlt_300'] = True\n",
    "task_valid.loc[task_data['altt_rt_pctlt_300'].astype(float)<pctl_bad_rt_cutoff,'altt_rt_pctlt_300'] = False\n",
    "task_valid.loc[task_data['altt_rt_pctlt_300'].isnull(),'altt_rt_pctlt_300'] =np.nan\n",
    "\n",
    "task_valid.loc[task_data['cvd_consp_rt_pctlt_300'].astype(float)>=pctl_bad_rt_cutoff,'cvd_consp_rt_pctlt_300'] = True\n",
    "task_valid.loc[task_data['cvd_consp_rt_pctlt_300'].astype(float)<pctl_bad_rt_cutoff,'cvd_consp_rt_pctlt_300'] = False\n",
    "task_valid.loc[task_data['cvd_consp_rt_pctlt_300'].isnull(),'cvd_consp_rt_pctlt_300'] = np.nan\n",
    "\n",
    "\n",
    "task_valid.loc[task_data['amp_pct_bad_rts'].astype(float)>=pctl_bad_rt_cutoff,'amp_pct_bad_rts'] = True\n",
    "task_valid.loc[task_data['amp_pct_bad_rts'].astype(float)<pctl_bad_rt_cutoff,'amp_pct_bad_rts'] = False\n",
    "task_valid.loc[task_data['amp_pct_bad_rts'].isnull(),'amp_pct_bad_rts'] = np.nan\n",
    "\n",
    "task_valid.loc[task_data['tr_1s_noVar']=='1.0','tr_1s_noVar'] = True\n",
    "task_valid.loc[task_data['tr_1s_noVar']!='1.0','tr_1s_noVar'] = False\n",
    "task_valid.loc[task_data['tr_1s_noVar'].isnull(),'tr_1s_noVar'] = np.nan\n",
    "\n",
    "task_valid.loc[task_data['iat_include']=='0.0','iat_exclude'] = True\n",
    "task_valid.loc[task_data['iat_include']!='0.0','iat_exclude'] = False\n",
    "task_valid.loc[task_data['iat_include'].isnull(),'iat_exclude'] = np.nan\n",
    "\n",
    "task_valid.loc[task_data['biat_include']=='0.0','biat_exclude'] = True\n",
    "task_valid.loc[task_data['biat_include']!='0.0','biat_exclude'] = False\n",
    "task_valid.loc[task_data['biat_include'].isnull(),'biat_exclude'] = np.nan\n",
    "\n",
    "task_valid['biat_missing'] = task_data.biat_missing\n",
    "task_valid['altt_missing'] = task_data.altt_missing\n",
    "task_valid['tr_1s_missing'] = task_data.tr_1s_missing\n",
    "task_valid['iat_missing'] = task_data.iat_missing\n",
    "task_valid['pgg_missing'] = task_data.pgg_missing\n",
    "task_valid['amp_missing'] = task_data.amp_missing\n",
    "task_valid['cvd_consp_missing'] = task_data.cvd_consp_missing\n",
    "\n",
    "task_valid['biat_administered'] = task_data.biat_administered\n",
    "task_valid['altt_administered'] = task_data.altt_administered\n",
    "task_valid['tr_1s_administered'] = task_data.tr_1s_administered\n",
    "task_valid['iat_administered'] = task_data.iat_administered\n",
    "task_valid['pgg_administered'] = task_data.pgg_administered\n",
    "task_valid['amp_administered'] = task_data.amp_administered\n",
    "task_valid['cvd_consp_administered'] = task_data.cvd_consp_administered\n",
    "\n",
    "# rename rt columns for consistency\n",
    "task_valid = task_valid.rename(columns={\"tr_1s_rt_pctlt_300\": \"tr_pct_bad_rts\",\n",
    "                                              \"altt_rt_pctlt_300\": \"altt_pct_bad_rts\",\n",
    "                                              \"cvd_consp_rt_pctlt_300\": \"cvd_consp_pct_bad_rts\"})\n",
    "\n",
    "# task missing/ administered \n",
    "tasks = ['altt','pgg','tr_1s','iat','amp','biat', 'cvd_consp']\n",
    "for w in task_valid.wave.unique():\n",
    "    for task in tasks:\n",
    "        if sum(task_valid.loc[task_valid.wave == w, task + '_administered'] == 'True')> 0:\n",
    "            task_valid.loc[task_valid.wave == w, task+ '_administered'] = 'True'\n",
    "        else:\n",
    "            task_valid.loc[task_valid.wave == w, task+ '_administered'] = 'False'\n",
    "        \n",
    "for task in tasks:\n",
    "    task_valid.loc[(task_valid[task+'_missing'].isnull()) &\n",
    "                                     (task_valid[task+'_administered'] == 'True'), task+'_missing'] = 'True'\n",
    "    task_valid.loc[(task_valid[task+'_missing'].isnull()) &\n",
    "                                     (task_valid[task+'_administered'] == 'False'), task+'_missing'] = 'False'\n",
    "# merge\n",
    "valid_data = valid_data.merge(task_valid, how = 'left', on = ['CVDID','wave'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# percentage of completed waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_prlfc = []\n",
    "for w in range(1,max_wave_prolific+1,1):\n",
    "    w_prlfc.append(str(w))\n",
    "\n",
    "valid_data_prlfc = valid_data.loc[valid_data['wave'].isin(w_prlfc),:].copy()\n",
    "valid_data_conte = valid_data.loc[valid_data['wave'].isin(w_conte),:].copy()\n",
    "del valid_data\n",
    "\n",
    "wave_count_prlfc = valid_data_prlfc[['CVDID','wave']].groupby('CVDID').count()  \n",
    "wave_count_prlfc = wave_count_prlfc.rename(columns={'wave': 'nCompleted'}).reset_index()\n",
    "wave_count_prlfc['perc_completed'] = wave_count_prlfc.nCompleted/wave_count_prlfc.nCompleted.max()\n",
    "valid_data_prlfc = valid_data_prlfc.merge(wave_count_prlfc, on = 'CVDID', how = 'left')\n",
    "\n",
    "wave_count_conte = valid_data_conte[['CVDID','wave']].groupby('CVDID').count()    \n",
    "wave_count_conte = wave_count_conte.rename(columns={'wave': 'nCompleted'})\n",
    "wave_count_conte['perc_completed'] = wave_count_conte.nCompleted/wave_count_conte.nCompleted.max()\n",
    "valid_data_conte = valid_data_conte.merge(wave_count_conte, on = 'CVDID', how = 'left')\n",
    "\n",
    "valid_data_prlfc['low_compl'] = valid_data_prlfc.perc_completed<0.5\n",
    "valid_data_conte['low_compl'] = valid_data_conte.perc_completed<0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weekly subject count based on (cummulative) quality criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_criteria =  ['low_compl', 'more_than_1_attQ_failed', 'string_outlier_all', 'string_outlier_core','duration_outlier',\n",
    "                            'string_outlier_amp', 'string_outlier_altt', 'tr_pct_bad_rts','response_consistency',\n",
    "                            'altt_pct_bad_rts', 'amp_pct_bad_rts', 'iat_exclude', 'biat_exclude','free_text_resp_valid_stress',\n",
    "                            'free_text_resp_valid_news','freq_NAresp']\n",
    "    \n",
    "# drop all rows of incomplete qualtrics data \n",
    "valid_data_prlfc = valid_data_prlfc.loc[valid_data_prlfc['completed']=='1',:]\n",
    "valid_data_conte = valid_data_conte.loc[valid_data_conte['completed']=='1',:]\n",
    "    \n",
    "valid_data_prlfc['perc_valid_failed'] = valid_data_prlfc[validation_criteria].mean(axis = 1, skipna = True)\n",
    "valid_data_conte['perc_valid_failed'] = valid_data_conte[validation_criteria].mean(axis = 1, skipna = True)\n",
    "valid_data_prlfc['N_valid_failed'] =  valid_data_prlfc[validation_criteria].sum(axis = 1, skipna = True)\n",
    "valid_data_conte['N_valid_failed'] = valid_data_conte[validation_criteria].sum(axis = 1, skipna = True)\n",
    "\n",
    "\n",
    "# wave-by-wave summary \n",
    "week_sub_prlfc = pd.DataFrame(index = valid_data_prlfc.wave.unique())\n",
    "for wave in list(week_sub_prlfc.index):\n",
    "    week_sub_prlfc.loc[wave, 'N sub'] = len(valid_data_prlfc.loc[valid_data_prlfc.wave == wave,:])\n",
    "    week_sub_prlfc.loc[wave, 'N QC'] = int(sum(valid_data_prlfc.loc[valid_data_prlfc.wave == wave,validation_criteria].isnull().sum(axis = 0) != week_sub_prlfc.loc[wave,'N sub']))\n",
    "    \n",
    "for wave in list(week_sub_prlfc.index):\n",
    "    for perc in list(np.arange(1, 0.2, -0.1)):\n",
    "        perc_pass = str(round(sum(valid_data_prlfc.loc[valid_data_prlfc.wave == wave,'perc_valid_failed']<=(1-perc))/sum(valid_data_prlfc.wave == wave)*100,2))+'%'\n",
    "        count_pass = ' (' + str( sum(valid_data_prlfc.loc[valid_data_prlfc.wave == wave,'perc_valid_failed']<=(1-perc))) + ')'\n",
    "        week_sub_prlfc.loc[wave,'> ' + str(round(perc*100))+'% QC passed'] = perc_pass +count_pass\n",
    "valid_data_prlfc.to_csv(os.path.join(data_dir, 'validation_passCriterion_perSub_perWave_w1-'+str(max_wave_prolific) + '_prlfc.csv'),index=False)\n",
    "\n",
    "week_sub_prlfc.to_csv(os.path.join(data_dir, 'validation_proportion_prct_pass_perWave_w1-'+str(max_wave_prolific) + '_prlfc.csv'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanResp_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVD-datapaper",
   "language": "python",
   "name": "cvd-datapaper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
